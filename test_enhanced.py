"""
Test script for enhanced LinkedIn Post Generator features
"""

import asyncio
import json
from post_generator import (
    create_linkedin_posts, 
    moderate_content, 
    apply_content_moderation,
    web_search,
    LLMClient
)

def test_content_moderation():
    """Test content moderation functionality"""
    print("ğŸ›¡ï¸ Testing Content Moderation...")
    
    # Test clean content
    clean_text = "This is a great post about artificial intelligence and machine learning."
    is_clean, reason = moderate_content(clean_text)
    print(f"âœ… Clean text: {is_clean} (reason: {reason})")
    
    # Test inappropriate content
    spam_text = "This contains spam and misleading information!!!"
    is_clean, reason = moderate_content(spam_text)
    print(f"âŒ Spam text: {is_clean} (reason: {reason})")
    
    # Test excessive caps
    caps_text = "THIS IS ALL CAPS AND LOOKS LIKE SPAM!"
    is_clean, reason = moderate_content(caps_text)
    print(f"âŒ Caps text: {is_clean} (reason: {reason})")

async def test_web_search():
    """Test web search functionality"""
    print("\nğŸ” Testing Web Search...")
    
    context = await web_search("artificial intelligence trends 2025", max_results=2)
    if context:
        print(f"âœ… Web search successful, found context:")
        print(f"Context preview: {context[:200]}...")
    else:
        print("âš ï¸ Web search returned no results")

def test_llm_metrics():
    """Test LLM client metrics tracking"""
    print("\nğŸ“Š Testing LLM Metrics...")
    
    client = LLMClient(provider="groq", api_key="")  # Will use mock
    
    # Simulate some API calls (mock responses)
    client.total_tokens = 150
    client.total_latency = 2.5
    client.call_count = 3
    
    metrics = client.get_metrics()
    print(f"âœ… Metrics: {json.dumps(metrics, indent=2)}")

async def test_enhanced_post_generation():
    """Test the full enhanced post generation"""
    print("\nğŸ¯ Testing Enhanced Post Generation...")
    
    # Test without web search
    result = await create_linkedin_posts(
        topic="machine learning",
        tone="professional",
        audience="data scientists",
        post_count=2,
        use_web_search=False,
        api_key=""
    )
    
    print(f"âœ… Generated {len(result['posts'])} posts")
    print(f"ğŸ“Š Metrics: {result['metrics']}")
    print(f"ğŸ” Used web search: {result['used_web_search']}")
    print(f"ğŸ“„ Context found: {result['context_found']}")
    
    # Show first post
    if result['posts']:
        first_post = result['posts'][0]
        print(f"\nğŸ“ Sample Post:")
        print(f"Text: {first_post['post_text'][:100]}...")
        print(f"Hashtags: {', '.join(first_post['hashtags'])}")
        print(f"CTA: {first_post['cta']}")

def test_post_moderation():
    """Test post moderation with sample data"""
    print("\nğŸ›¡ï¸ Testing Post Moderation...")
    
    # Sample posts with one that should be moderated
    sample_posts = [
        {
            "post_text": "Great insights on AI development!",
            "hashtags": ["#AI", "#Tech"],
            "cta": "What do you think?"
        },
        {
            "post_text": "This is spam content with misleading information!",
            "hashtags": ["#spam"],
            "cta": "Click here!"
        }
    ]
    
    moderated_posts = apply_content_moderation(sample_posts)
    
    for i, post in enumerate(moderated_posts):
        print(f"Post {i+1}: {'âœ… Clean' if 'moderated' not in post['post_text'].lower() else 'âŒ Moderated'}")

async def main():
    """Run all tests"""
    print("ğŸ§ª Running Enhanced LinkedIn Post Generator Tests\n")
    
    # Test individual components
    test_content_moderation()
    await test_web_search()
    test_llm_metrics()
    test_post_moderation()
    
    # Test full integration
    await test_enhanced_post_generation()
    
    print("\nğŸ‰ All tests completed!")

if __name__ == "__main__":
    asyncio.run(main())
